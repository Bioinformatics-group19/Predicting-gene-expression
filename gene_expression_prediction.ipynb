{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4235d04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pandas==2.2.3\n",
      "  Downloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas==2.2.3)\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas==2.2.3)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas==2.2.3)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.3)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "\u001b[2K  Attempting uninstall: pytz\n",
      "\u001b[2K    Found existing installation: pytz 2024.1\n",
      "\u001b[2K    Uninstalling pytz-2024.1:\n",
      "\u001b[2K      Successfully uninstalled pytz-2024.1\n",
      "\u001b[2K  Attempting uninstall: tzdata━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K    Found existing installation: tzdata 2023.3━━━━\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K    Uninstalling tzdata-2023.3:━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K      Successfully uninstalled tzdata-2023.3━━━━━━\u001b[0m \u001b[32m0/6\u001b[0m [pytz]\n",
      "\u001b[2K  Attempting uninstall: six\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K    Found existing installation: six 1.16.04m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K    Uninstalling six-1.16.0:[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [tzdata]\n",
      "\u001b[2K      Successfully uninstalled six-1.16.08;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [six]zdata]\n",
      "\u001b[2K  Attempting uninstall: numpy━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [six]\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: python-dateutil━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: python-dateutil 2.9.0.post08;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling python-dateutil-2.9.0.post0:;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled python-dateutil-2.9.0.post0[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: pandas━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Found existing installation: pandas 2.2.30m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K    Uninstalling pandas-2.2.3:━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K      Successfully uninstalled pandas-2.2.3\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━\u001b[0m \u001b[32m4/6\u001b[0m [python-dateutil]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [pandas]━━━━\u001b[0m \u001b[32m5/6\u001b[0m [pandas]on-dateutil]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jaxlib 0.9.0.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "jax 0.9.0.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.31.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall --no-cache-dir numpy==1.26.4 pandas==2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee817483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f921755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chromosomes loaded: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5']\n"
     ]
    }
   ],
   "source": [
    "genome_file = \"GRCh38.primary_assembly.genome.fa\"\n",
    "genome = SeqIO.to_dict(SeqIO.parse(genome_file, \"fasta\"))\n",
    "print(\"Chromosomes loaded:\", list(genome.keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22287c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of genes: 78899\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_id</th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>strand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000290825.2</td>\n",
       "      <td>chr1</td>\n",
       "      <td>11121</td>\n",
       "      <td>24894</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000223972.6</td>\n",
       "      <td>chr1</td>\n",
       "      <td>12010</td>\n",
       "      <td>13670</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000310526.1</td>\n",
       "      <td>chr1</td>\n",
       "      <td>14356</td>\n",
       "      <td>30744</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000227232.6</td>\n",
       "      <td>chr1</td>\n",
       "      <td>14696</td>\n",
       "      <td>24886</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000278267.1</td>\n",
       "      <td>chr1</td>\n",
       "      <td>17369</td>\n",
       "      <td>17436</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gene_id chrom  start    end strand\n",
       "0  ENSG00000290825.2  chr1  11121  24894      +\n",
       "1  ENSG00000223972.6  chr1  12010  13670      +\n",
       "2  ENSG00000310526.1  chr1  14356  30744      -\n",
       "3  ENSG00000227232.6  chr1  14696  24886      -\n",
       "4  ENSG00000278267.1  chr1  17369  17436      -"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gtf_file = \"gencode.v49.primary_assembly.basic.annotation.gtf\"\n",
    "genes = []\n",
    "\n",
    "with open(gtf_file) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"): continue\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if fields[2] != \"gene\": continue\n",
    "        chrom = fields[0]\n",
    "        start = int(fields[3])\n",
    "        end = int(fields[4])\n",
    "        strand = fields[6]\n",
    "        # Parse gene_id from attributes\n",
    "        attr = fields[8]\n",
    "        gene_id = attr.split('gene_id \"')[1].split('\"')[0]\n",
    "        genes.append([gene_id, chrom, start, end, strand])\n",
    "\n",
    "genes_df = pd.DataFrame(genes, columns=[\"gene_id\",\"chrom\",\"start\",\"end\",\"strand\"])\n",
    "print(\"Number of genes:\", len(genes_df))\n",
    "genes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plus_bw = pyBigWig.open(\"K562_plus_unique.bigWig\")\n",
    "minus_bw = pyBigWig.open(\"K562_minus_unique.bigWig\")\n",
    "# showing the first few lines of the bigwig files\n",
    "print(\"Plus bigwig header:\", plus_bw.header())\n",
    "print(\"Minus bigwig header:\", minus_bw.header())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d4b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr1', 'chr2', 'chr3', 'chr4', 'chr5']\n",
      "['chr1', 'chr2', 'chr3', 'chr4', 'chr5']\n"
     ]
    }
   ],
   "source": [
    "print(list(genome.keys())[:5])\n",
    "print(list(plus_bw.chroms().keys())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f6796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot(seq):\n",
    "    mapping = np.zeros(128, dtype=np.int8)  # ASCII table\n",
    "    mapping[ord('A')] = 0\n",
    "    mapping[ord('C')] = 1\n",
    "    mapping[ord('G')] = 2\n",
    "    mapping[ord('T')] = 3\n",
    "    \n",
    "    seq = seq.upper()\n",
    "    arr = np.zeros((len(seq),4), dtype=np.float32)\n",
    "    # only convert A/C/G/T\n",
    "    valid_idx = np.array([ord(b) for b in seq])\n",
    "    arr[np.arange(len(seq)), mapping[valid_idx]] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7783a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved batch 0 with 10000 genes\n",
      "Saved batch 1 with 10000 genes\n",
      "Saved batch 2 with 10000 genes\n",
      "Saved batch 3 with 10000 genes\n",
      "Saved batch 4 with 10000 genes\n",
      "Saved batch 5 with 10000 genes\n",
      "Saved batch 6 with 10000 genes\n",
      "Saved batch 7 with 8691 genes\n"
     ]
    }
   ],
   "source": [
    "window = 5000\n",
    "seq_len = 2 * window\n",
    "batch_size = 10000  # adjust if needed\n",
    "\n",
    "# Keep only genes whose chromosomes exist in the bigWigs\n",
    "primary_genes_df = genes_df[genes_df['chrom'].isin(plus_bw.chroms().keys())].reset_index(drop=True)\n",
    "genes = primary_genes_df.to_dict('records')\n",
    "\n",
    "# Integer encoding function\n",
    "def int_encode(seq):\n",
    "    \"\"\"Encode DNA sequence as integers: A=0, C=1, G=2, T=3, N/other=4\"\"\"\n",
    "    seq = seq.upper()\n",
    "    mapping = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "    return np.array([mapping.get(b, 4) for b in seq], dtype=np.int64)\n",
    "\n",
    "for i in range(0, len(genes), batch_size):\n",
    "    batch_genes = genes[i:i+batch_size]\n",
    "    X_batch = []\n",
    "    y_batch = []\n",
    "    \n",
    "    for row in batch_genes:\n",
    "        chrom = row['chrom']\n",
    "        strand = row['strand']\n",
    "        tss = row['start'] if strand == '+' else row['end']\n",
    "        \n",
    "        chrom_len = plus_bw.chroms()[chrom]\n",
    "        seq_start = max(0, tss - window)\n",
    "        seq_end = min(chrom_len, tss + window)\n",
    "        \n",
    "        seq = genome[chrom].seq[seq_start:seq_end]\n",
    "        if strand == '-':\n",
    "            seq = seq.reverse_complement()\n",
    "        \n",
    "        # Integer encoding with padding\n",
    "        seq_array = int_encode(str(seq))\n",
    "        if len(seq_array) < seq_len:\n",
    "            seq_array = np.pad(seq_array, (0, seq_len - len(seq_array)), mode='constant', constant_values=4)\n",
    "        X_batch.append(seq_array)\n",
    "        \n",
    "        # CAGE signal\n",
    "        plus_signal = np.nan_to_num(plus_bw.values(chrom, seq_start, seq_end, numpy=True))\n",
    "        minus_signal = np.nan_to_num(minus_bw.values(chrom, seq_start, seq_end, numpy=True))\n",
    "        total_signal = plus_signal + minus_signal\n",
    "        if len(total_signal) < seq_len:\n",
    "            total_signal = np.pad(total_signal, (0, seq_len - len(total_signal)), mode='constant')\n",
    "        y_batch.append(total_signal.sum())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_batch = np.array(X_batch, dtype=np.int64)  # integers\n",
    "    y_batch = np.array(y_batch, dtype=np.float32)  # labels\n",
    "\n",
    "    # Save batch compressed to save space\n",
    "    np.savez_compressed(f\"data_batches/data_batch_{i//batch_size}.npz\", X=X_batch, y=y_batch)\n",
    "    print(f\"Saved batch {i//batch_size} with {len(batch_genes)} genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3019ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(primary_genes_df) // batch_size\n",
    "class GeneDataset(Dataset):\n",
    "    def __init__(self, batch_files):\n",
    "        self.files = batch_files\n",
    "        self.index_map = []\n",
    "        # build index mapping to know which batch/file contains which sample\n",
    "        for b, f in enumerate(batch_files):\n",
    "            data = np.load(f)\n",
    "            for i in range(len(data['y'])):\n",
    "                self.index_map.append((b, i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx, i = self.index_map[idx]\n",
    "        data = np.load(self.files[batch_idx])\n",
    "        X = data['X'][i]\n",
    "        y = np.log1p(data['y'][i])  # log1p here\n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "dataset = GeneDataset([f\"data_batches/data_batch_{i}.npz\" for i in range(num_batches)])\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c0e9e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m val_loader   \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m test_loader  \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_batch\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my batch shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_batch\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mGeneDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_idx, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_map[idx]\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[batch_idx])\n\u001b[0;32m---> 18\u001b[0m X \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m     19\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog1p(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m][i])  \u001b[38;5;66;03m# log1p here\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(X), torch\u001b[38;5;241m.\u001b[39mtensor(y)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mread_array(\u001b[38;5;28mbytes\u001b[39m,\n\u001b[1;32m    257\u001b[0m                              allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_pickle,\n\u001b[1;32m    258\u001b[0m                              pickle_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpickle_kwargs,\n\u001b[1;32m    259\u001b[0m                              max_header_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_header_size)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 831\u001b[0m             data \u001b[38;5;241m=\u001b[39m _read_bytes(fp, read_size, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/numpy/lib/format.py:966\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         r \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m    967\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/zipfile/__init__.py:989\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 989\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read1(n)\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/zipfile/__init__.py:1057\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m-> 1057\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read2(n \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read2(n)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/zipfile/__init__.py:1089\u001b[0m, in \u001b[0;36mZipExtFile._read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1086\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m   1087\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left)\n\u001b[0;32m-> 1089\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileobj\u001b[38;5;241m.\u001b[39mread(n)\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/zipfile/__init__.py:803\u001b[0m, in \u001b[0;36m_SharedFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 803\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing():\n\u001b[1;32m    804\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read from the ZIP file while there \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    805\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an open writing handle on it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    806\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose the writing handle before trying to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/zipfile/__init__.py:1617\u001b[0m, in \u001b[0;36mZipFile.open.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;66;03m# Open for reading:\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fileRefCnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1616\u001b[0m zef_file \u001b[38;5;241m=\u001b[39m _SharedFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp, zinfo\u001b[38;5;241m.\u001b[39mheader_offset,\n\u001b[0;32m-> 1617\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fpclose, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing)\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     \u001b[38;5;66;03m# Skip the file header:\u001b[39;00m\n\u001b[1;32m   1620\u001b[0m     fheader \u001b[38;5;241m=\u001b[39m zef_file\u001b[38;5;241m.\u001b[39mread(sizeFileHeader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(\"X batch shape:\", X_batch.shape)  # [batch_size, seq_len]\n",
    "print(\"y batch shape:\", y_batch.shape)  # [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2490b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f53a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, pool_size, channels):\n",
    "        super().__init__()\n",
    "        self.pool_size = pool_size\n",
    "        self.weight_proj = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, L)\n",
    "        B, C, L = x.shape\n",
    "        weights = self.weight_proj(x) # (B, C, L)\n",
    "        \n",
    "        x = x.view(B, C, L // self.pool_size, self.pool_size)\n",
    "        weights = weights.view(B, C, L // self.pool_size, self.pool_size)\n",
    "        \n",
    "        weights = torch.softmax(weights, dim=-1)\n",
    "        return (x * weights).sum(dim=-1)\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(channels, channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ConvTowerBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = ConvBlock(in_channels, out_channels, kernel_size=5)\n",
    "        self.res_block = ResidualConvBlock(out_channels)\n",
    "        self.pool = AttentionPool(pool_size=2, channels=out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block(x)\n",
    "        x = self.res_block(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class RelativePosition(nn.Module):\n",
    "    def __init__(self, num_units, max_relative_position):\n",
    "        super().__init__()\n",
    "        self.num_units = num_units\n",
    "        self.max_relative_position = max_relative_position\n",
    "        self.embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, num_units))\n",
    "        nn.init.xavier_uniform_(self.embeddings_table)\n",
    "\n",
    "    def forward(self, length, device):\n",
    "        range_vec = torch.arange(length, device=device)\n",
    "        distance_mat = range_vec[None, :] - range_vec[:, None]\n",
    "        distance_mat_clipped = torch.clamp(distance_mat, -self.max_relative_position, self.max_relative_position)\n",
    "        final_mat = distance_mat_clipped + self.max_relative_position\n",
    "        final_mat = final_mat.long()\n",
    "        embeddings = self.embeddings_table[final_mat]\n",
    "        return embeddings\n",
    "\n",
    "class MultiHeadAttentionRel(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, key_size=64):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.key_size = key_size\n",
    "        self.inner_dim = key_size * num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(d_model, self.inner_dim)\n",
    "        self.k_proj = nn.Linear(d_model, self.inner_dim)\n",
    "        self.v_proj = nn.Linear(d_model, self.inner_dim)\n",
    "        self.out_proj = nn.Linear(self.inner_dim, d_model)\n",
    "        \n",
    "        # Relative positional encoding\n",
    "        self.rel_pos = RelativePosition(key_size, max_relative_position=256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, C = x.shape\n",
    "        \n",
    "        q = self.q_proj(x).view(B, L, self.num_heads, self.key_size).transpose(1, 2) # (B, H, L, K)\n",
    "        k = self.k_proj(x).view(B, L, self.num_heads, self.key_size).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(B, L, self.num_heads, self.key_size).transpose(1, 2)\n",
    "        \n",
    "        # q @ k.T -> (B, H, L, L)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.key_size ** 0.5)\n",
    "        \n",
    "        # Relative positional encoding\n",
    "        rel_pos_embed = self.rel_pos(L, x.device) # (L, L, K)\n",
    "        \n",
    "        rel_scores = torch.einsum('bhlk,lmk->bhlm', q, rel_pos_embed) / (self.key_size ** 0.5)\n",
    "        \n",
    "        scores = scores + rel_scores\n",
    "        \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        out = torch.matmul(attn, v) # (B, H, L, K)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, L, self.inner_dim)\n",
    "        \n",
    "        return self.out_proj(out)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model=1536, num_heads=8, key_size=64, ff_hidden=6144):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttentionRel(d_model, num_heads, key_size)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_hidden, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, C)\n",
    "        x = x + self.mha(self.norm1(x))\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class Enformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Initial feature extraction and downsampling\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(4, 768, kernel_size=15, padding=7),\n",
    "            ResidualConvBlock(768),\n",
    "            AttentionPool(pool_size=2, channels=768)\n",
    "        )\n",
    "        \n",
    "        # 2. Hierarchical feature extraction + downsampling (Conv Tower)\n",
    "        # 6 blocks\n",
    "        tower_channels = [768, 896, 1024, 1152, 1280, 1408, 1536]\n",
    "        self.conv_tower = nn.Sequential(*[\n",
    "            ConvTowerBlock(tower_channels[i], tower_channels[i+1])\n",
    "            for i in range(len(tower_channels)-1)\n",
    "        ])\n",
    "        \n",
    "        # 3. Transformer Block (x11 layers)\n",
    "        self.transformers = nn.Sequential(*[\n",
    "            TransformerBlock(d_model=1536, num_heads=8, key_size=64, ff_hidden=6144)\n",
    "            for _ in range(11)\n",
    "        ])\n",
    "        \n",
    "        # 4. Pointwise Block\n",
    "        self.pointwise = nn.Sequential(\n",
    "            nn.Conv1d(1536, 3072, kernel_size=1),\n",
    "            nn.Dropout(0.05),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 5. Output Heads (Human head)\n",
    "        self.human_head = nn.Sequential(\n",
    "            nn.Conv1d(3072, 5313, kernel_size=1),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (B, 4, 196608)\n",
    "        \n",
    "        # Stem\n",
    "        x = self.stem(x) # Output: (B, 768, 98304)\n",
    "        \n",
    "        # Conv Tower\n",
    "        x = self.conv_tower(x) # Output: (B, 1536, 1536)\n",
    "        \n",
    "        # Transformer expects (B, L, C)\n",
    "        x = x.transpose(1, 2) # Output: (B, 1536, 1536)\n",
    "        x = self.transformers(x) # Output: (B, 1536, 1536)\n",
    "        x = x.transpose(1, 2) # Output: (B, 1536, 1536)\n",
    "        \n",
    "        # Crop (Remove edges: 1536 -> 896)\n",
    "        # (1536 - 896) / 2 = 320\n",
    "        x = x[:, :, 320:-320] # Output: (B, 1536, 896)\n",
    "        \n",
    "        # Pointwise Block\n",
    "        x = self.pointwise(x) # Output: (B, 3072, 896)\n",
    "        \n",
    "        # Human Head\n",
    "        x = self.human_head(x) # Output: (B, 5313, 896)\n",
    "        \n",
    "        # Final Output shape: (B, 896, 5313)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "# Example usage:\n",
    "# model = Enformer()\n",
    "# x = torch.randn(1, 4, 196608)\n",
    "# y = model(x)\n",
    "# print(\"Output shape:\", y.shape) # Expected: torch.Size([1, 896, 5313])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c60c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To initialize weights from the pre-trained Enformer model, we can use the `enformer-pytorch` library.\n",
    "# First, make sure it's installed: %pip install enformer-pytorch\n",
    "\n",
    "from enformer_pytorch import Enformer as PretrainedEnformer\n",
    "\n",
    "def load_pretrained_weights(custom_model):\n",
    "    print(\"Loading pre-trained Enformer model...\")\n",
    "    # Load the official pre-trained weights\n",
    "    pretrained_model = PretrainedEnformer.from_pretrained('EleutherAI/enformer-official-rough')\n",
    "    \n",
    "    print(\"Transferring weights to custom model...\")\n",
    "    \n",
    "    # 1. Transfer Stem weights\n",
    "    custom_model.stem[0].weight.data = pretrained_model.stem[0].weight.data.clone()\n",
    "    custom_model.stem[0].bias.data = pretrained_model.stem[0].bias.data.clone()\n",
    "    \n",
    "    # Residual Conv Block in Stem\n",
    "    custom_model.stem[1].conv[0].weight.data = pretrained_model.stem[1].fn[0].weight.data.clone()\n",
    "    custom_model.stem[1].conv[0].bias.data = pretrained_model.stem[1].fn[0].bias.data.clone()\n",
    "    custom_model.stem[1].conv[2].weight.data = pretrained_model.stem[1].fn[2].weight.data.clone()\n",
    "    custom_model.stem[1].conv[2].bias.data = pretrained_model.stem[1].fn[2].bias.data.clone()\n",
    "    \n",
    "    # Attention Pool in Stem\n",
    "    custom_model.stem[2].weight_proj.weight.data = pretrained_model.stem[2].pool_fn.weight.data.clone()\n",
    "    custom_model.stem[2].weight_proj.bias.data = pretrained_model.stem[2].pool_fn.bias.data.clone()\n",
    "\n",
    "    # 2. Transfer Conv Tower weights\n",
    "    for i in range(6):\n",
    "        # ConvBlock\n",
    "        custom_model.conv_tower[i].conv_block.conv[0].weight.data = pretrained_model.conv_tower[i][0].fn[0].weight.data.clone()\n",
    "        custom_model.conv_tower[i].conv_block.conv[0].bias.data = pretrained_model.conv_tower[i][0].fn[0].bias.data.clone()\n",
    "        custom_model.conv_tower[i].conv_block.conv[2].weight.data = pretrained_model.conv_tower[i][0].fn[2].weight.data.clone()\n",
    "        custom_model.conv_tower[i].conv_block.conv[2].bias.data = pretrained_model.conv_tower[i][0].fn[2].bias.data.clone()\n",
    "        \n",
    "        # ResidualConvBlock\n",
    "        custom_model.conv_tower[i].res_block.conv[0].weight.data = pretrained_model.conv_tower[i][1].fn[0].weight.data.clone()\n",
    "        custom_model.conv_tower[i].res_block.conv[0].bias.data = pretrained_model.conv_tower[i][1].fn[0].bias.data.clone()\n",
    "        custom_model.conv_tower[i].res_block.conv[2].weight.data = pretrained_model.conv_tower[i][1].fn[2].weight.data.clone()\n",
    "        custom_model.conv_tower[i].res_block.conv[2].bias.data = pretrained_model.conv_tower[i][1].fn[2].bias.data.clone()\n",
    "        \n",
    "        # AttentionPool\n",
    "        custom_model.conv_tower[i].pool.weight_proj.weight.data = pretrained_model.conv_tower[i][2].pool_fn.weight.data.clone()\n",
    "        custom_model.conv_tower[i].pool.weight_proj.bias.data = pretrained_model.conv_tower[i][2].pool_fn.bias.data.clone()\n",
    "\n",
    "    # 3. Transfer Transformer weights\n",
    "    for i in range(11):\n",
    "        # LayerNorms\n",
    "        custom_model.transformers[i].norm1.weight.data = pretrained_model.transformer[i][0].norm.weight.data.clone()\n",
    "        custom_model.transformers[i].norm1.bias.data = pretrained_model.transformer[i][0].norm.bias.data.clone()\n",
    "        custom_model.transformers[i].norm2.weight.data = pretrained_model.transformer[i][1].norm.weight.data.clone()\n",
    "        custom_model.transformers[i].norm2.bias.data = pretrained_model.transformer[i][1].norm.bias.data.clone()\n",
    "        \n",
    "        # MHA\n",
    "        custom_model.transformers[i].mha.q_proj.weight.data = pretrained_model.transformer[i][0].fn.to_q.weight.data.clone()\n",
    "        custom_model.transformers[i].mha.k_proj.weight.data = pretrained_model.transformer[i][0].fn.to_k.weight.data.clone()\n",
    "        custom_model.transformers[i].mha.v_proj.weight.data = pretrained_model.transformer[i][0].fn.to_v.weight.data.clone()\n",
    "        custom_model.transformers[i].mha.out_proj.weight.data = pretrained_model.transformer[i][0].fn.to_out.weight.data.clone()\n",
    "        \n",
    "        # Relative Positional Encoding\n",
    "        custom_model.transformers[i].mha.rel_pos.embeddings_table.data = pretrained_model.transformer[i][0].fn.rel_pos.rel_pos.data.clone()\n",
    "        \n",
    "        # FFN\n",
    "        custom_model.transformers[i].ffn[0].weight.data = pretrained_model.transformer[i][1].fn.net[0].weight.data.clone()\n",
    "        custom_model.transformers[i].ffn[0].bias.data = pretrained_model.transformer[i][1].fn.net[0].bias.data.clone()\n",
    "        custom_model.transformers[i].ffn[2].weight.data = pretrained_model.transformer[i][1].fn.net[3].weight.data.clone()\n",
    "        custom_model.transformers[i].ffn[2].bias.data = pretrained_model.transformer[i][1].fn.net[3].bias.data.clone()\n",
    "\n",
    "    # 4. Transfer Pointwise Block weights\n",
    "    custom_model.pointwise[0].weight.data = pretrained_model.crop_and_pointwise[1].weight.data.clone()\n",
    "    custom_model.pointwise[0].bias.data = pretrained_model.crop_and_pointwise[1].bias.data.clone()\n",
    "\n",
    "    # 5. Transfer Human Head weights\n",
    "    # Note: The official model has a slightly different head structure, but we can transfer the final linear layer\n",
    "    custom_model.human_head[0].weight.data = pretrained_model._heads['human'][0].weight.data.clone()\n",
    "    custom_model.human_head[0].bias.data = pretrained_model._heads['human'][0].bias.data.clone()\n",
    "\n",
    "    print(\"Weights transferred successfully!\")\n",
    "    return custom_model\n",
    "\n",
    "# Example usage:\n",
    "# model = Enformer()\n",
    "# model = load_pretrained_weights(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
